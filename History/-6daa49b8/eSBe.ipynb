{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8feedfd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dsar\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from ds_helper import load_table, combine_records, split_by_feature, get_timestamp,get_feature_importance, find_corr, cal_metrics\n",
    "\n",
    "# timestamp = datetime.today().strftime('%Y%m%d%H%M%S')\n",
    "cohort_include_smoker=False\n",
    "\n",
    "lr_model_table = 'hb.hb_poc_premiums_rv3'\n",
    "pred_out_table = 'hb.uw_sim_preds_v1'\n",
    "\n",
    "if cohort_include_smoker:\n",
    "    out_name_cohort = 's' #smoker\n",
    "else:\n",
    "    out_name_cohort = 'ns' #No smoker\n",
    "\n",
    "model_id = os.environ.get('MODEL_ID', 178)\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "pd.set_option('display.max_rows', 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eff24ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "useful_cols = [\n",
    "# 'app_age',\n",
    "# 'ins_sex',\n",
    "'first_pol_substandard',\n",
    "'first_year',\n",
    "'contract_no',\n",
    "# 'annual_prem',\n",
    "'adj_annual_prem',\n",
    "'exposure',\n",
    "'year_seq',\n",
    "'risk_class',\n",
    "'tot_clm_amt', 'tot_cnt', 'tot_pay_amt',\n",
    "'inflate_tot_pay_amt','inflate_tot_clm_amt','inflate_tot_cnt']\n",
    "\n",
    "columns = ', '.join(useful_cols) if useful_cols else '*'\n",
    "with dsar.psql_con(\"HB\") as con:\n",
    "    df = pd.read_sql(sql=f\"\"\"select {columns} from {lr_model_table}\n",
    "                            where plan_region='HK' and vhis_compatible=1\n",
    "                            and year_seq<=5 and risk_class!='decline'\n",
    "                            order by contract_no, year_seq\n",
    "\"\"\", con = con)\n",
    "\n",
    "df = df.drop(['tot_clm_amt', 'tot_cnt'], axis=1)\n",
    "df = df.rename({\n",
    "    'inflate_tot_clm_amt': 'tot_clm_amt', \n",
    "    'inflate_tot_cnt': 'tot_cnt', \n",
    "#     'inflate_tot_pay_amt': 'tot_pay_amt',\n",
    "    'adj_annual_prem': 'annual_prem'\n",
    "    }, axis=1)\n",
    "\n",
    "\n",
    "if 'risk_class' in df:\n",
    "    df['risk_class'] = df.risk_class.replace(['exclusion','exclusion_loading', 'loading'],'substandard') \n",
    "    \n",
    "# NOTE: whether the sum is calculated from the raw claims or adjusted claims are determined by the variable `clm_col_end`\n",
    "clm_col_end = '_clm_amt' # the ending of the claim columns. This could be '_clm_amt' or '_clm_amt_adj', depending on whether we want to use the inflation-adjusted version or not. \n",
    "\n",
    "unwanted_claim_cols = [\n",
    "    'hom_nur_clm_amt', 'acc_clm_amt', 'smm_dr_clm_amt', \n",
    "    'cancer_clm_amt', 'smm_rb_clm_amt', 'ic_clm_amt', \n",
    "    'hi_clm_amt', 'tot_clm_amt'\n",
    "    ] \n",
    "\n",
    "# ------------------ define extra variables for convenience ------------------ #\n",
    "claim_cols = df.columns[df.columns.str.contains(clm_col_end)].to_list() # variable `claim_cols` contains all the columns related to claims\n",
    "\n",
    "claim_cols_pruned = [col for col in claim_cols if col not in unwanted_claim_cols] # this variable removes those unwanted claim columns\n",
    "# ---------------------------------------------------------------------------- #\n",
    "\n",
    "# print some info about the dataset\n",
    "print(f'n_records = {df.shape[0]}')\n",
    "print(f'n_features = {df.shape[1]}')\n",
    "print(f'n_claim_records = {df.tot_clm_amt.gt(0).sum()}')\n",
    "print(f'n_unique_contracts = {df.contract_no.nunique()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c5cc4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the effective premium paid during the exposure period. This column is needed when making decile plots for aggregate loss ratio\n",
    "df['eff_annual_prem'] = df['annual_prem'] * df['exposure']\n",
    "\n",
    "# ---------------------------- define clean_record --------------------------- #\n",
    "# A record is clean if it does not have any disease history\n",
    "# df['clean_record'] = (~df[disease_groups].any(axis=1)).astype('int64')\n",
    "\n",
    "# ----------------------------- define loss_ratio ---------------------------- # \n",
    "# there are two possible choices: either use tot_clm_amt or tot_pay_amt, but tot_pay_amt is preferred \n",
    "\n",
    "# df['loss_ratio'] = (df.tot_clm_amt / (df.annual_prem * df.exposure)) # use tot_clm_amt\n",
    "df['loss_ratio'] = (df.tot_pay_amt / (df.annual_prem * df.exposure)) # loss ratio within the insured's exposure \n",
    "# df['loss_ratio'] = (df.tot_pay_amt/df.exposure) / df.annual_prem # loss ratio per exposure. But this is in fact the same as above\n",
    "\n",
    "# --- define frequency and severity for the frequency*severity formulation --- #\n",
    "df['frequency'] = df['tot_cnt'] / df['exposure']\n",
    "df['severity'] = df['loss_ratio']/np.fmax(df['frequency'], 1)\n",
    "# df['severity'] = df['loss_ratio']/np.fmax(df['tot_cnt'], 1)\n",
    "# ---------------------------------------------------------------------------- #\n",
    "\n",
    "# ---------------------- for binary classification model --------------------- #\n",
    "df['has_clm'] = df['tot_cnt'].gt(0)\n",
    "# ---------------------------------------------------------------------------- #\n",
    "\n",
    "# sanity check for the calculation of frequency and severity\n",
    "df.query('loss_ratio>0 and exposure !=1.0')[['exposure', 'tot_pay_amt', 'annual_prem', 'tot_cnt' , 'loss_ratio','frequency', 'severity', 'has_clm']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b3d800",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lr_mapping = combine_records(df)\n",
    "df_lr_mapping = df_lr_mapping[df_lr_mapping.exposure==5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953cf932",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lr_mapping.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747edeee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.risk_class.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896bf457",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lr_mapping.risk_class.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3013aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lr_mapping = df_lr_mapping.drop(['risk_class'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6527760",
   "metadata": {},
   "source": [
    "# LR model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304cba54",
   "metadata": {},
   "outputs": [],
   "source": [
    "with dsar.psql_con(\"HB\") as con:\n",
    "    lr_out = pd.read_sql(sql=f\"\"\"select out.*, raw.risk_class, raw.app_age, raw.ins_sex from {pred_out_table} out                             \n",
    "                            left join \n",
    "                            (select distinct contract_no, risk_class, app_age, ins_sex from {lr_model_table}\n",
    "                            where risk_class is not null and year_seq=1) raw\n",
    "                            on raw.contract_no = out.contract_no\n",
    "                            where model_id = '{model_id}'\"\"\", con = con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73f91dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692dd190",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_out = pd.merge(lr_out, df_lr_mapping, how='left', on='contract_no')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c9b9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_out = lr_out[(lr_out['risk_class']=='decline')|(lr_out['exposure']==5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15469146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ convert `ins_sex` to indicator variable ----------------- #\n",
    "lr_out['sex_male'] = lr_out.ins_sex.map({'M':1, 'F':0})\n",
    "lr_out = lr_out.drop('ins_sex', axis='columns')\n",
    "    \n",
    "age_gp_interval = list(range(0, 56, 5)) + [np.inf]\n",
    "# age_gp_interval = [-1,10,20,30,40,50,np.inf]\n",
    "# # labels = ['baby','kid','teen','adult','mid-age','senior']\n",
    "lr_out['age_gp'] = pd.cut(lr_out['app_age'], age_gp_interval, right=True, include_lowest=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7139f099",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_out.query('risk_class==\"decline\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2d9fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_out.risk_class.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d58e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "if cohort_include_smoker: \n",
    "    cohort_col = ['age_gp','sex_male','smoker']\n",
    "else:\n",
    "    cohort_col = ['age_gp','sex_male']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe934f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_out['cohort'] = lr_out[cohort_col].astype(str).agg('|'.join, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f8e307",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_out['cohort'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce53790",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_out_test = lr_out[lr_out.train.astype(int)==0]\n",
    "lr_out_train = lr_out[lr_out.train.astype(int)==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542f238b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_out_train.groupby(cohort_col).size().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb3ae1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_out_test.groupby(cohort_col).size().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b45d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr_thresholds_xgb = (np.percentile(lr_out[lr_out.train_1==1]['xgb'], np.linspace(1,100,100)))\n",
    "# lr_thresholds_freq = (np.percentile(lr_out[lr_out.train_1==1]['freq_severity'], np.linspace(1,100,100)))\n",
    "# lr_thresholds_cla = (np.percentile(lr_out[lr_out.train_1==1]['cla_reg'], np.linspace(1,100,100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1764e271",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_extra_class_size(df):\n",
    "    decline_size = sum(df['risk_class']=='decline')\n",
    "    exclusion_size = sum(df['risk_class'].isin(['exclusion', 'exclusion_loading']))\n",
    "    loading_size = sum(df['risk_class'].isin(['loading', 'exclusion_loading']))\n",
    "    std_size = sum(pd.to_numeric(df['first_pol_substandard'], errors='coerce')==0)\n",
    "    return decline_size, exclusion_size, loading_size, std_size\n",
    "\n",
    "def get_class_loss_stat(df):\n",
    "    pay = df['tot_pay_amt'].sum()\n",
    "    prem = df['eff_annual_prem'].sum()\n",
    "    size = len(df)\n",
    "    return pay, prem, size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441e4f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_cohort_percentile(lr_out_test,lr_out_train, pred_col_name):\n",
    "    '''\n",
    "    Returns dataframe with loss ratio base on different loss ratio threshold by cohort.\n",
    "\n",
    "            Parameters:\n",
    "                    lr_out_test (pd.DataFrame): test set with output for loss model\n",
    "                    lr_out_train (pd.DataFrame): train set with output for loss model\n",
    "                    pred_col_name (str): column name for predicted value (predicted loss ratio)\n",
    "                    true_col_name (str): column name for true label (true loss ratio)\n",
    "            Returns:\n",
    "                    result of different thresholds\n",
    "                    threshold for each cohort\n",
    "    '''\n",
    "    cohort_threshold_dict={}\n",
    "    lr_cohort_test = pd.DataFrame()\n",
    "    lr_cohort_train = pd.DataFrame()\n",
    "    for cohort in lr_out_test['cohort'].unique(): #loop each cohort\n",
    "        lr_subset_train = lr_out_train.loc[lr_out_train['cohort']==cohort].copy()\n",
    "        lr_subset_test = lr_out_test.loc[lr_out_test['cohort']==cohort].copy()\n",
    "        if len(lr_subset_test)==0 or len(lr_subset_train)==0:\n",
    "            continue\n",
    "        else:\n",
    "            lr_thresholds = np.percentile(lr_subset_train[pred_col_name],\n",
    "                                          np.linspace(1,99,99)) #find percentile from train\n",
    "            lr_subset_test.loc[:,'cohort_percentile'] = np.digitize(lr_subset_test[pred_col_name],\n",
    "                                                              lr_thresholds, right=True)+1 #map percentile to test\n",
    "            lr_subset_train.loc[:,'cohort_percentile'] = np.digitize(lr_subset_train[pred_col_name],\n",
    "                                                              lr_thresholds, right=True)+1 #map percentile to train\n",
    "            lr_cohort_test = pd.concat([lr_cohort_test,lr_subset_test], axis=0) #merge all cohort together\n",
    "            lr_cohort_train = pd.concat([lr_cohort_train,lr_subset_train], axis=0) #merge all cohort together\n",
    "            cohort_threshold_dict[cohort] = lr_thresholds\n",
    "    \n",
    "    return lr_cohort_train,lr_cohort_test, cohort_threshold_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1eb8fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_out_train, lr_out_test,cohort_threshold_dict = map_cohort_percentile(lr_out_test,lr_out_train, 'pred')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662bc0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_out_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20649837",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_percentile_class(srs, pref_percentile, std_percentile):\n",
    "    if srs<=pref_percentile:\n",
    "        return 'pref'\n",
    "    elif srs<=std_percentile:\n",
    "        return 'std'\n",
    "    else:\n",
    "        return 'substd'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2f3c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_out_test['class'] = lr_out_test['cohort_percentile'].apply(map_percentile_class, args=(10,91))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94209965",
   "metadata": {},
   "outputs": [],
   "source": [
    "age_gp_interval = list(range(0, 56, 10)) + [np.inf]\n",
    "age_gp_interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ad4ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "age_gp_interval_origin = list(range(0, 56, 5)) + [np.inf]\n",
    "age_gp_interval_large = [0, 20, 30, 40, 50, np.inf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44c289f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def twin_lineplot(x,y,**kwargs):\n",
    "    ax = plt.twinx()\n",
    "    sns.pointplot(x=x,y=y,**kwargs, ax=ax)\n",
    "    ax.grid(visible=False)\n",
    "    ax.set_ylabel('record count')\n",
    "\n",
    "        \n",
    "def plot_all(df_lr, grpby_sex, age_bin=age_gp_interval_origin, model_id=model_id, out=False):\n",
    "    if out:\n",
    "        directory = f'./model_{model_id}'\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "    df = df_lr\n",
    "    hasgender = 'Y' if grpby_sex else 'N'\n",
    "    df['age_gp'] = pd.cut(df['app_age'], age_bin, right=True, include_lowest=True)\n",
    "    unique_bin_n = df['age_gp'].nunique()\n",
    "    df['n_record'] = 1\n",
    "    df['decline_cnt'] = (df.risk_class=='decline').astype(int)\n",
    "    by = ['age_gp','class']\n",
    "    if grpby_sex:\n",
    "        by.append('sex_male')\n",
    "    diff_df = df.groupby(by)[['tot_pay_amt','eff_annual_prem','n_record']].sum().assign(\n",
    "                                loss_ratio = lambda df: df['tot_pay_amt']/df['eff_annual_prem']).unstack(1)\n",
    "    diff_df['pref_std_diff'] = (diff_df['loss_ratio']['pref']/diff_df['loss_ratio']['std']-1)\n",
    "    diff_df['std_substd_diff'] = (diff_df['loss_ratio']['std']/diff_df['loss_ratio']['substd']-1)\n",
    "    by_no_class = [col for col in by if col !='class']\n",
    "    diff_df.columns = diff_df.columns.map(lambda x: '_'.join(x).strip('_'))\n",
    "#     diff_df['n_record'] = diff_df[['n_record_pref', 'n_record_std', 'n_record_substd']].sum(axis=1)\n",
    "    diff_df = diff_df.reset_index()\n",
    "    by_no_class = [col for col in by if col !='class']\n",
    "    diff_df = pd.melt(diff_df, id_vars=by_no_class + ['n_record_pref', 'n_record_std', 'n_record_substd'],\n",
    "                      value_vars=['pref_std_diff','std_substd_diff'],\n",
    "        var_name='diff_type', value_name='prop_diff', ignore_index=False)\n",
    "    diff_df['n_record'] = diff_df.apply(lambda x:\n",
    "                                   x['n_record_pref'] + x['n_record_std'] if x['diff_type'] == 'pref_std_diff'\n",
    "                                  else x['n_record_substd'] + x['n_record_std'], axis=1)\n",
    "    plot_df = df.groupby(by)[['decline_cnt','n_record','tot_pay_amt','eff_annual_prem']].sum().assign(\n",
    "        decline_prop = lambda df: df['decline_cnt']/df['n_record'],\n",
    "        loss_ratio = lambda df: df['tot_pay_amt']/df['eff_annual_prem']).reset_index()\n",
    "    \n",
    "    g = sns.catplot(data=plot_df, x='age_gp', y='decline_prop', row='sex_male' if grpby_sex else None,\n",
    "                    col='class', kind='bar', sharex=False, sharey=True,)\n",
    "    g.map(twin_lineplot,'age_gp','n_record')\n",
    "    g.set_xticklabels(rotation=30, ha=\"right\")\n",
    "    g.set_ylabels('decline ratio')\n",
    "    g.figure.suptitle('decline ratio')\n",
    "    g.tight_layout() \n",
    "    if out:\n",
    "        g.savefig(os.path.join(directory,f'model{model_id}_bin{unique_bin_n}_gender{hasgender}_decline.jpg'), dpi = 250)\n",
    "    g = sns.catplot(data=plot_df, x='age_gp', y='loss_ratio', row='sex_male' if grpby_sex else None,\n",
    "                    col='class', kind='bar', sharex=False, sharey=True)\n",
    "    g.map(twin_lineplot,'age_gp','n_record')\n",
    "    g.set_xticklabels(rotation=30, ha=\"right\")\n",
    "    g.set_ylabels('loss ratio')\n",
    "    g.figure.suptitle('loss ratio')\n",
    "    g.tight_layout() \n",
    "    if out:\n",
    "        g.savefig(os.path.join(directory,f'model{model_id}_bin{unique_bin_n}_gender{hasgender}_loss.jpg'), dpi = 250)\n",
    "        \n",
    "    g = sns.catplot(data=diff_df, x='age_gp', y='prop_diff', row='sex_male' if grpby_sex else None,\n",
    "               col = 'diff_type',\n",
    "               kind='bar',\n",
    "               sharex=False, sharey=True)\n",
    "    g.map(twin_lineplot,'age_gp','n_record')\n",
    "    g.set_ylabels('class risk differentiation')\n",
    "    g.set_xticklabels(rotation=30, ha=\"right\")\n",
    "    g.figure.suptitle('class risk differentiation')\n",
    "    g.tight_layout() \n",
    "    if out:\n",
    "        g.savefig(os.path.join(directory,f'model{model_id}_bin{unique_bin_n}_gender{hasgender}_propdiff.jpg'), dpi = 250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c946f2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all(lr_out_test, grpby_sex = False , age_bin = age_gp_interval_large, model_id=model_id, out=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8647337f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all(lr_out_test, grpby_sex = False , age_bin = age_gp_interval_origin, model_id=model_id, out=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930622dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all(lr_out_test, grpby_sex = True , age_bin = age_gp_interval_large, model_id=model_id, out=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70131dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all(lr_out_test, grpby_sex = True , age_bin = age_gp_interval_origin, model_id=model_id, out=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d6b6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_out_test.groupby(['age_gp']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0cc9eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_df = lr_out_test.groupby(['age_gp','class','sex_male'])[['tot_pay_amt','eff_annual_prem', 'n_record']].sum().assign(\n",
    "                                loss_ratio = lambda df: df['tot_pay_amt']/df['eff_annual_prem']).unstack(1)\n",
    "diff_df['pref_std_diff'] = (diff_df['loss_ratio']['pref']/diff_df['loss_ratio']['std']-1)\n",
    "diff_df['std_substd_diff'] = (diff_df['loss_ratio']['std']/diff_df['loss_ratio']['substd']-1)\n",
    "by_no_class = [col for col in ['age_gp','class','sex_male'] if col !='class']\n",
    "diff_df.columns = diff_df.columns.map(lambda x: '_'.join(x).strip('_'))\n",
    "diff_df['n_record'] = diff_df[['n_record_pref', 'n_record_std', 'n_record_substd']].sum(axis=1)\n",
    "diff_df = diff_df.reset_index()\n",
    "# diff_df = diff_df[['age_gp','sex_male','pref_std_diff','std_substd_diff']].droplevel(1, axis=1) \n",
    "diff_df = pd.melt(diff_df, id_vars=['age_gp','sex_male']+ ['n_record'], value_vars=['pref_std_diff','std_substd_diff'],\n",
    "        var_name='diff_type', value_name='prop_diff', ignore_index=False)\n",
    "diff_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227966dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_threshold_range(lr_out_test, pref_min, pref_max, std_min, std_max):\n",
    "    test_stat = lr_out_test.cohort_percentile.value_counts(normalize=True).rename('proportion').sort_index().reset_index()\n",
    "    test_stat['cum_prop'] = test_stat['proportion'].cumsum()\n",
    "    test_stat = test_stat.rename(columns={'index':'percentile'})\n",
    "    pref_stat = test_stat.query(f'{pref_min}<=cum_prop<={pref_max}')\n",
    "    pref_min_p, pref_max_p = pref_stat['percentile'].min(), pref_stat['percentile'].max()\n",
    "    std_stat = test_stat.query(f'{std_min}<=cum_prop<={std_max}')\n",
    "    std_min_p, std_max_p = std_stat['percentile'].min(), std_stat['percentile'].max()\n",
    "    return pref_min_p, pref_max_p, std_min_p, std_max_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2241919d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pref_min_p, pref_max_p, std_min_p, std_max_p  = get_threshold_range(lr_out_test, 0.07, 0.13, 0.87, 0.93)\n",
    "# pref_min_p, pref_max_p, std_min_p, std_max_p  = get_threshold_range(lr_out_test, 0.14, 0.13, 0.87, 0.93)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1771fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "pref_min_p, pref_max_p, std_min_p, std_max_p "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2416b440",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_lr_threshold(lr_out_test, pred_col_name,pref_min_p,pref_max_p, std_min_p, std_max_p):\n",
    "    lr_evaluation=[]\n",
    "    for i in range(pref_min_p,pref_max_p+1):\n",
    "        pref_percentile = i\n",
    "        pref = lr_out_test[lr_out_test[pred_col_name]<=i]\n",
    "        pref_pay, pref_prem, pref_size = get_class_loss_stat(pref)\n",
    "        pref_decline, pref_exclusion, pref_loading, pref_std = get_extra_class_size(pref)\n",
    "        for j in range(max(std_min_p,pref_max_p+1),std_max_p+1):\n",
    "            std_percentile = j\n",
    "            std = lr_out_test[(lr_out_test[pred_col_name]>i) & \n",
    "                                     (lr_out_test[pred_col_name]<=j)]\n",
    "            std_pay, std_prem, std_size = get_class_loss_stat(std)\n",
    "            std_decline, std_exclusion, std_loading, std_std = get_extra_class_size(std)\n",
    "            S_P_test = lr_out_test[lr_out_test[pred_col_name]<=j]\n",
    "            if len(S_P_test)==0:\n",
    "                agreement_rate=0\n",
    "            else:\n",
    "                agreement_rate = sum(pd.to_numeric(S_P_test['first_pol_substandard'], errors='coerce')==0)/len(S_P_test)\n",
    "            \n",
    "            substd = lr_out_test[(lr_out_test[pred_col_name]>j)]\n",
    "            substd_pay, substd_prem, substd_size = get_class_loss_stat(substd)\n",
    "            substd_decline, substd_exclusion, substd_loading, substd_std = get_extra_class_size(substd)\n",
    "            if len(substd)==0:\n",
    "                substd_agreement_rate=0\n",
    "            else:\n",
    "                substd_agreement_rate = sum(pd.to_numeric(substd['first_pol_substandard'], errors='coerce')!=0)/len(substd)\n",
    "            threshold_result = [pref_percentile, std_percentile, pref_size,\n",
    "                                std_size,substd_size, \n",
    "                                agreement_rate, substd_agreement_rate,\n",
    "                               pref_decline, pref_exclusion, pref_loading,\n",
    "                               std_decline, std_exclusion, std_loading,\n",
    "                               substd_decline, substd_exclusion, substd_loading,\n",
    "                               pref_pay,std_pay,substd_pay,\n",
    "                               pref_prem, std_prem, substd_prem,\n",
    "                               pref_std, std_std, substd_std]\n",
    "            lr_evaluation.append(threshold_result)\n",
    "\n",
    "    lr_evaluation = pd.DataFrame(lr_evaluation, columns = ['pref_percentile','std_percentile',\n",
    "                                                           'pref_size','std_size','substd_size',\n",
    "                                                           'agreement_rate','nstd_agreement_rate',\n",
    "                                                          'pref_decline', 'pref_exclusion', 'pref_loading',\n",
    "                                                          'std_decline', 'std_exclusion', 'std_loading',\n",
    "                                                          'substd_decline', 'substd_exclusion', 'substd_loading',\n",
    "                                                          'pref_pay','std_pay','substd_pay',\n",
    "                                                           'pref_prem', 'std_prem', 'substd_prem',\n",
    "                                                          'pref_std', 'std_std', 'substd_std'])  \n",
    "    return lr_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca8dec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_process_lr_eva(lr_eva):\n",
    "    table = lr_eva\n",
    "    table['pref_prop'] = table['pref_size']/table[['pref_size','std_size','substd_size']].sum(axis=1)\n",
    "    table['std_prop'] = table['std_size']/table[['pref_size','std_size','substd_size']].sum(axis=1)\n",
    "    table['substd_prop'] = table['substd_size']/table[['pref_size','std_size','substd_size']].sum(axis=1)\n",
    "#     pref_size = (table['pref_size']-table['pref_decline'])\n",
    "#     std_size = (table['std_size']-table['std_decline'])\n",
    "#     substd_size = (table['substd_size']-table['substd_decline'])\n",
    "    for risk_class in ['pref', 'std', 'substd']:\n",
    "        table[f'{risk_class}_loss'] = (table[f'{risk_class}_pay']).divide(table[f'{risk_class}_prem'])\n",
    "        table[f'{risk_class}_decline_prop'] = (table[f'{risk_class}_decline']).divide(table[f'{risk_class}_size'])\n",
    "    table['S_P_loss'] = (table[f'pref_pay'] +\n",
    "                         table[f'std_pay']).divide(table[f'pref_prem']+\n",
    "                                                             table[f'std_prem'])\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7f524b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_threshold_to_df(threshold_dict): \n",
    "    threshold_df = pd.DataFrame()\n",
    "    for k, v in threshold_dict.items():\n",
    "        cohort_threshold = pd.Series(v).reset_index()\n",
    "        cohort_threshold.columns =['precentile', 'threshold']\n",
    "        cohort_threshold['precentile'] = cohort_threshold['precentile']+1\n",
    "        cohort_threshold['cohort'] = k\n",
    "        threshold_df = pd.concat([threshold_df,cohort_threshold], axis=0)\n",
    "    threshold_df[cohort_col] =   threshold_df['cohort'].str.split('|',expand=True)\n",
    "    threshold_df = threshold_df.drop('cohort', axis=1)\n",
    "    return threshold_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1eda4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_col = ['model_id', 'pref_percentile', 'std_percentile', 'pref_prop', 'std_prop','substd_prop',\n",
    "           'pref_loss', 'std_loss',\n",
    "           'substd_loss','S_P_loss','agreement_rate', 'nstd_agreement_rate',\n",
    "        'pref_decline_prop', 'std_decline_prop', 'substd_decline_prop',\n",
    "        'pref_size', 'std_size','substd_size',\n",
    "       'pref_decline', 'pref_exclusion', 'pref_loading',\n",
    "      'std_decline', 'std_exclusion', 'std_loading',\n",
    "    'substd_decline', 'substd_exclusion', 'substd_loading',\n",
    "    'pref_std', 'std_std', 'substd_std'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ebd70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if not pd.Series([pref_min_p,pref_max_p, std_min_p, std_max_p]).isnull().any():\n",
    "#     eva_out = evaluate_lr_threshold(lr_out_test, 'cohort_percentile',pref_min_p,pref_max_p, std_min_p, std_max_p)\n",
    "#     eva_out['model_id'] = model_id\n",
    "#     eva_out = post_process_lr_eva(eva_out)\n",
    "#     cohort_threshold_df =  parse_threshold_to_df(cohort_threshold_dict)\n",
    "#     cohort_threshold_df['model_id'] =  model_id\n",
    "#     with dsar.psql_con('HB') as con:\n",
    "#         eva_out[out_col].to_sql(name=f'uw_sim_tune', con=con, schema='hb',index=False, if_exists='append')\n",
    "#     #     cohort_threshold_df.to_sql(name=f'{out_name}', con=con, schema='hb',index=False, if_exists='append')\n",
    "# else:\n",
    "#     os.write(1, f\"model_id: {model_id}, no suitable threshold found\\n\".encode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c0200a",
   "metadata": {},
   "outputs": [],
   "source": [
    "eva_out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_science",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4 | packaged by conda-forge | (main, Mar 24 2022, 17:42:03) [Clang 12.0.1 ]"
  },
  "vscode": {
   "interpreter": {
    "hash": "ae74f9feb07f97b665e59d852dca9947bf3c6be9bdf551f43d711a8fd00af3ab"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
