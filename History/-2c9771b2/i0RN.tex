\documentclass[10pt, oneside]{article}   
% \documentclass{article}   
% \usepackage{C:/Users/whlin/Documents/Latex/Styles/should_be_useless/My_Style/Assignment}
\usepackage{/Users/whlin/Library/CloudStorage/OneDrive-HKUSTConnect/Documents/Latex/Styles/My_Style/MyStyle}
\usepackage{booktabs} % To thicken table lines
\usepackage{tabularx}
\usepackage{pbox}
\usepackage{enumitem}
\usepackage{tabularx, makecell}%
\renewcommand\theadfont{\normalsize\bfseries}
\usepackage{etoolbox} %
\AtBeginEnvironment{tabularx}{\setlist[itemize, 1]{wide, leftmargin=*, itemsep=0pt, before=\vspace{-\dimexpr\baselineskip +2 \partopsep}, after=\vspace{-\baselineskip}}}
%\end{comment}
%%%%%
%If pdftex is used, then the subbookmarks won't expand
%%%%

%\hypersetup{
%	colorlinks=true,
%	citecolor=Violet,
%	linkcolor=Red,
%	urlcolor=blue}
\usepackage{tabularx}
\usepackage{float}
\usepackage{longtable}
\usepackage{bibentry}
\setlength{\voffset}{-0.25in}
\makeatletter\let\saved@bibitem\@bibitem\makeatother
% If the doc. is for printing, use the segment to change all colors to black
\begin{comment}
\hypersetup{colorlinks, citecolor=black, filecolor=black, linkcolor=black, urlcolor=black, pdftex}
\end{comment}

%\usepackage{titlesec}
%\usepackage{lipsum}
% \usepackage{bibentry}
% \makeatletter\let\saved@bibitem\@bibitem\makeatother
\def \B {\mathcal{B}}
\def \W {\mathcal{W}}

\def \b {\textbf}

%\titleformat{\section}
%{\normalfont\scshape}{\thesection}{1em}{}
%\title{Assignment}
%\author{Student Name: Joe Wing-Ho Lin\\ Student ID: 20137952}
%\date{}

%\parindent 2em 
\linespread{0.89}
\hyphenpenalty=10000

%\renewcommand{\baselinestretch}{0.9}



\begin{document}
\nobibliography{Pub.bib}
%	\bibliographystyle{unsrt}
%\maketitle 
%\tableofcontents  %show table of content
%\newpage
%\begin{multicols*}{2}  % the "*" means the lengths of the two columns can differ. note that additional "[]" can be used to make introductory paragargh to the two columns 
%\large{ Lin Wing Ho }
\begin{center}
\section*{Wing Ho Lin}
% \section*{\huge{Wing Ho Lin}}
\begin{comment}
\begin{tabular}{p{2.1cm}l}	
	\toprule
%	Address: & 13/F, Kam Sing Building
%	88A, Prince Edward Road West,
%	Prince Edward, Hong Kong\\
	Telephone: & +852 64336027\\
	Email: & \href{mailto:whlinaa@cse.ust.hk}
	{whlinaa@cse.ust.hk}
%	Research Interests: & Database base and data mining. 
\end{tabular}	
\end{comment}
% Assistant Lecturer, College of Professional and Continuing Education (CPCE), Hong Kong Polytechnic University\\
% Assistant Lecturer, Hong Kong Community College (HKCC), Hong Kong Polytechnic University\\
% Tel: (852) 6433-6027 \qquad Email: \href{mailto:whlinaa@connect.ust.hk}{whlinaa@connect.ust.hk} \qquad Homepage:  \href{https://www.hkcc-polyu.edu.hk/en/about-hkcc/staff-directory/division-of-science-engineering-and-health-studies/index.php?sid=318}{here} \qquad GitHub: \href{https://github.com/whlinaa?tab=repositories}{here}
Tel: (852) 6433-6027 \qquad Email: \href{mailto:whlinaa@connect.ust.hk}{whlinaa@connect.ust.hk} 
% \qquad GitHub: \href{https://github.com/whlinaa?tab=repositories}{here}

% Tel: +852 64336027 \qquad Email: \href{mailto:lkj872001@gmail.com}{lkj872001@gmail.com} \qquad Homepage:  \href{https://www.hkcc-polyu.edu.hk/en/about-hkcc/staff-directory/division-of-science-engineering-and-health-studies/index.php?sid=318}{here}
\end{center}
%\vspace{-0.2cm}
% \section*{Areas of Interests}
% Numerical Linear Algebra, 
% Machine Learning, Data Mining, Neural Networks.
% support vector machines, data mining.
% optimization, matrix factorization,
\vspace{-0.8cm}
\section*{Education}
\begin{tabularx}{\linewidth}{p{2.2cm}|p{17cm}}
	\toprule 
\textsc{~Sep} 2015 |  & Master of Philosophy in Computer Science (Research Area: Data Mining)\\
\textsc{Aug} 2017 & Hong Kong University of Science and Technology (HKUST)\\
& GPA: \b{3.965} (4-point scale)
% & Thesis Title: Frequent Item Finding when Obtaining Support is Costly 
%(received \textbf{Champion Award }of Postgraduate Paper Competition from IEEE Computation Intelligence Chapter)\\
% & Supervisor: Prof. \href{https://www.cse.ust.hk/~raywong/}{Raymond Chi-Wing Wong}\\
% & \small{Thesis received \textbf{Champion Award }of Postgraduate Paper Competition from IEEE Computation Intelligence Chapter}\\
\\\\
\textsc{Jun} 2017 |  & International Summer School Program \\
\textsc{Jul} 2017& Peking University (PKU), Beijing, China
	\\& Overall score: \textbf{88.55}/100
%\textsc{Present}& Peking University (PKU), Beijing, China	
\\\\
\textsc{Sep} 2013 |  & Bachelor of Engineering in Computer Science (First Class Honors)\\
\textsc{Jul} 2015& Hong Kong University of Science and Technology (HKUST)\\
& GPA: \b{3.881} (4-point scale) %(in top 1.5\% of all students graduated in 2015)} 
\\
% &\href{https://drive.google.com/open?id=1ZCn4DEuqxciS6mTMCwEY44IbwPbFbu3G}{Ranked 1st among all 48 students in the BEng in Computer	Science Program}
&\b{Ranked {1\textsuperscript{st}} among all graduates in the same program} (confirmation letter \href{https://drive.google.com/open?id=1ZCn4DEuqxciS6mTMCwEY44IbwPbFbu3G}{\underline{here}})
\\\\
\textsc{Sep} 2011 |  & Associate in Statistics and Computing for Business (Distinction)\\
\textsc{Jul} 2013& Hong Kong Community College (HKCC), Hong Kong Polytechnic University\\
& GPA: \b{4.00} (4-point scale)\\
&Ranked 1\textsuperscript{st} and the only one with 4.0 GPA among all graduates in the same program (confirmation letter \href{https://drive.google.com/file/d/1cgY5f74OeQwvsQzqbZbWYSRvBQIEI66W/view?usp=sharing}{\underline{here}})
\end{tabularx}
\vspace*{-0.5cm}
\section*{Work Experience}
\begin{tabularx}{\linewidth}{p{2.2cm}|p{16cm}}
	\toprule 

	\textsc{~Sep} 2021 | & \textbf{Data Scientist}\\
	\textsc{Dec 2022} & MassMutual Financial Group\\
	& Built machine-learning models (e.g., XGBoost, generalized linear models) to predict insurance claim amount, loss ratio, claim frequency and automate the insurance policy underwriting process.
	% Visualized the experiment results using Pyplot (a third-party package in Python).
		% under the supervision of Prof. \href{https://www.cse.ust.hk/~raywong/}{\underline{Raymond Chi-Wing Wong}}.
	\\\\

	 % (full-time)
	\textsc{Aug} 2017 | & \textbf{Assistant Lecturer (Data Science)}  (Full-time: Aug 2017 -- Sep 2021; Part-time: Sep 2021 -- Present) \\
	\textsc{Present} & College of Professional and Continuing Education, Hong Kong Polytechnic University\\
	% \textsc{Present} & Hong Kong Community College, Hong Kong Polytechnic University\\
	% & $-$ Lead a research project to implement a real-time emotion recognition system to detect students' emotions to aid online teaching (prototype can be found \href{https://github.com/whlinaa/emotion-detection}{\underline{here}}).\\
	% & $-$ Implement a real-time emotion recognition system to detect students' emotions to aid online teaching (a prototype can be found here).\\
	% & $-$ Design teaching materials for various courses, including data mining and Python programming. \\
	% & $-$ Teach lectures and tutorials of undergraduate courses in data mining (using Python), programming in Python and C++, data structures, statistics and mathematics (e.g., linear algebra and calculus):\\
	% & $-$ Teach lectures and tutorials of undergraduate courses for data science and computer science students:\\
	& Teach lectures and tutorials of undergraduate courses in data science and computer science:\\
	&	\begin{itemize}
		\vspace{0.1cm}
		\footnotesize{
			\item SEHH3164: Introduction to Data Mining (Spring 19, Spring 20, Fall 20, Fall 21)
			% : a hands-on course focusing on classification and regression models. Python and data science packages (e.g., numpy, pandas, matplotlib and scikit-learn) are extensively used throughout the course.
			% \item SEHH3163: Introduction to Big Data Analytics (Fall 21)
			% \item SEHH2240: Database Systems (Spring 21, Fall 21)
			\item SEHH2042: Computer Programming in Python (Fall 18, Spring 20, Spring 21)
			\item SEHH2311: Statistical Inference (Spring 23)
			\item SEHH2239: Data Structures (Spring 22, Summer 22)
			\item SEHH2241: Discrete Structures (Fall 18, Fall 19, Fall 20, Spring 21, Fall 21, Fall 22, Spring 23)
			\item SEHH1070: Statistics and Vector Algebra (Spring 21)
			\item SEHH1069: Calculus and Linear Algebra (Spring 19, Spring 23)
			\item SEHH1048: Introduction to Linear Algebra (Fall 17, Spring 18, Spring 19)
			\item SEHH1050: Introduction to Probability and Statistics (Fall 17)		
			\item SEHH1068: Foundation Mathematics (Fall 19)		
			\item Workshop in Python Programming (Winter 17, Winter 18)
			\item Workshop in Calculus (Fall 17, Spring 18)
			\item Workshop in Mathematics for Data Science (Winter 22)
			\item Workshop in Statistical Computation in Python for Data Science (Winter 22)
			% \item Instructor at Math Learning Center
		}
	% &	\begin{itemize}
	% 	\vspace{0.1cm}
	% 	\small{
	% 		\item SEHH3164: Introduction to Data Mining (Spring 19, Spring 20, Fall 20): a hands-on course focusing on supervised and unsupervised learning algorithms. Python and data science packages (e.g., NumPy, Pandas, Matplotlib and Scikit-learn) are used throughout the course to 
	% 		\item SEHH2042: Computer Programming (Fall 18, Spring 20, Spring 21)
	% 		\item SEHH2241: Discrete Structures (Fall 18, Fall 19, Fall 20, Spring 21)
	% 		\item SEHH1070: Statistics and Vector Algebra (Spring 21)
	% 		\item SEHH1069: Calculus and Linear Algebra (Spring 19)
	% 		\item SEHH1048: Introduction to Linear Algebra (Fall 17, Spring 18, Spring 19)
	% 		\item SEHH1050: Introduction to Probability and Statistics (Fall 17)		
	% 		\item SEHH1068: Foundation Mathematics (Fall 19)		
	% 		\item Programming Workshop (Winter 17, Winter 18)
	% 		\item Calculus Workshop (Fall 17, Spring 18)
	% 		\item Instructor at Math Learning Center
	% 	}
	\end{itemize}
	% &	\begin{itemize}
	% 	\vspace{0.1cm}
	% 	\small{
	% 		\item SEHH1048: Introduction to Linear Algebra (Fall 17, Spring 18, Spring 19)
	% 		\item SEHH1050: Introduction to Probability and Statistics (Fall 17)		
	% 		\item SEHH1068: Foundation Mathematics (Fall 19)		
	% 		\item SEHH1069: Calculus and Linear Algebra (Spring 19)
	% 		\item SEHH1070: Statistics and Vector Algebra (Spring 21)
	% 		\item SEHH2042: Computer Programming (Fall 18, Spring 20, Spring 21)
	% 		\item SEHH2241: Discrete Structures (Fall 18, Fall 19, Fall 20, Spring 21)
	% 		\item SEHH3164: Introduction to Data Mining (Spring 19, Spring 20, Fall 20)
	% 		\item Programming Workshop (Winter 17, Winter 18)
	% 		\item Calculus Workshop (Fall 17, Spring 18)
	% 		\item Instructor at Math Learning Center
	% 	}
	% \end{itemize}
	\\\\
	% \textsc{Aug} 2017 | & \textbf{Visiting Lecturer} \\
	% \textsc{Aug} 2018 & Hong Kong Community College, Hong Kong Polytechnic University\\	
	% &	\begin{itemize}
	% 	\vspace{0.1cm}
	% 	\small{
	% 		\item SEHH1048: Introduction to Linear Algebra (Fall 17, Spring 18)
	% 		\item SEHH1050: Introduction to Probability and Statistics (Fall 17)		
	% 		\item Programming Workshop (Winter 17)
	% 		\item Calculus Workshop (Fall 17, Spring 18)
	% 		\item Tutor at Math Learning Center
	% 		}
	% \end{itemize}
	% \\\\

	\textsc{~Sep} 2017 | & \textbf{Research Assistant} (part-time)\\
	\textsc{Mar} 2018 & Department of Computer Science and Engineering, HKUST\\
	& Implemented data mining algorithms in research papers (e.g., Reverse Top-$k$ Threshold Algorithm, Sticky Sampling); undertook experiments to compare their performance with our proposed algorithm using Python and C++; visualized the research outcomes using Matplotlib.
	% Visualized the experiment results using Pyplot (a third-party package in Python).
		% under the supervision of Prof. \href{https://www.cse.ust.hk/~raywong/}{\underline{Raymond Chi-Wing Wong}}.
	\\\\
	% (part-time)
	\textsc{Feb} 2016 | & \textbf{Teaching Assistant} \\
	\textsc{Jun} 2017 & Department of Computer Science and Engineering, HKUST\\
	& Taught tutorials, designed test questions and marked students' work of computer science courses: \\
	&	\begin{itemize}
		\vspace{0.1cm}
		\footnotesize{
			\item COMP1942: Exploring and Visualizing Data (Spring 17)
			\item COMP3511: Database Management Systems (Spring 16)
			\item COMP2711: Discrete Mathematical Tools for Computer Science (Fall 16) %\vspace{0.1cm}\\\footnotesize{Teaching tutorials, grading quizzes \& exams, and holding consultation sessions. }
			%\vspace{0.1cm}\\\footnotesize{Teaching labs, setting homework \& exam questions, grading homework \& exams, and holding consultation sessions. }
		}
	\end{itemize}
\end{tabularx}

\section*{Honors and Awards}
%\vspace{ -.4 cm}
%\begin{table}
%\begin{tabular}{p{2.1cm}|p{14cm}}
%\begin{longtable}{l|p{16cm}}
\begin{tabularx}{\linewidth}{p{2.2cm}|p{16cm}}
	\toprule 
%\hline
	\pbox{2.2cm}{\small{IEEE, Computational Intelligence Chapter}} &   \pbox{15cm}{{\textbf{Champion Award of Postgraduate Student Research Paper Competition}} (2017) \\ 	\small{Awarded to one student each year based on the quality of the research paper submitted to the competition as well as the performance of the ensuing paper presentation.}}

	
\\\\
%	\hline
	\pbox{2.1cm}{Education Bureau, Hong Kong} & \pbox{16cm} {HKSAR Government Scholarship -- Continuing Studentsâ€™ Awards (all years of undergraduate study)  \\ \small{Awarded to HKUST students with an overall GPA $\ge 3.85$, a demonstrated leadership capacity, and a good command of English, among other criteria, to fully cover their undergraduate tuition fee.}}	


	\\\\
%		\hline
%		\hline
	& Outstanding Performance Scholarship (2013) \\
	& \small{Awarded to students with outstanding academic performance, valuable contributions to the school, and good communication skills, among other criteria.}
	\\\\
	HKUST 
	& Li Po Chun Charitable Trust Fund Scholarships (2016-17)\\
	&	\small{Awarded to three postgraduate students each year according to research progress and academic performance.}
\\\\
	& Postgraduate Studentship (all semesters of enrollment)\\
	& \small{Awarded to postgraduate research students to fully cover their program fees and living expenses.}
	\\\\
	& \textbf{Outstanding Student Award} (2014--15)\\
	& \small{For an elite group of \b{68 out of 4482 ($\approx 1.5\%$)} graduating students  with an excellent academic record.}
	\\\\
	& Dean's List (all semesters of enrollment)\\
	&\small{For students with a term grade average $\ge3.70$.}
	\\\\
%	& Certificate of Achievement for Graduate Teaching Assistant Training Program (2016)\\
%	&\small{For graduate teaching assistants who successfully completed at least six teaching-related training courses.}
%	\\\\ 
	HKCC & Outstanding Student Award Scholarship (Gold) (2011--12) \\
	&\small{For students with outstanding academic achievements during their first year of study.}
	\\\\
	& Outstanding Student Award Scholarship: Department of Applied Mathematics (2011--12) \\
	&\small{For students pursuing a degree in a mathematics-related field and with strong academic performance.}
	\\\\

	&  Director's List (2011--12)\\
	&\small{For students with excellent academic performance in the whole academic year.}
	\\\\
	& Dean's List (all semesters of enrollment) \\
	&\small{For students ranked among the top 5\% of all registered students in the semester.}
\end{tabularx}
%\end{tabular}
%\end{table}

\begin{comment}
\section*{Certificate}
\begin{tabular}{p{2.1cm}|p{13cm}}	
	\toprule 
	Hong Kong Statistical Society & Higher Certificate\\
	& \footnotesize{TODO}
\end{tabular}
\end{comment}
%\newpage




\section*{Language and Technical Skills}
%\vspace{ -.4 cm}
%\begin{table}
%\begin{tabular}{p{2.1cm}|p{14cm}}
%\begin{longtable}{l|p{16cm}}
\begin{tabularx}{\linewidth}{p{2.2cm}|p{16cm}}
	\toprule 
	Languages
	% \pbox{2.1cm}{Languages} 
	& \begin{itemize}
		\item English (Fluent. IELTS overall band score: 8.0 out of 9.0, taken in Oct 2019)
		\item Chinese (Native)
		% \item Mandarine (Intermediate)
	\end{itemize}
	\\\\
	Programming Languages
	% \pbox{2.1cm}{Programming languages}
	& \begin{itemize}
		\item Python (Proficient)
		\item Data science packages, such as NumPy, Pandas, Matplotlib, Scikit-learn, Keras, and TensorFlow (Proficient)
		\item SQL (Proficient)
		% \item C++ (Familiar)
	\end{itemize}
	\\\\
	Machine Learning
	% \pbox{2.1cm}{Machine learning}
	& \begin{itemize}
		\item Proficient in deep learning models, such as Multilayer Perceptron (MLP), Convolutional Neural Network (CNN), and Recurrent Neural Network (RNN).
		\item Solid grasp on hyperparameter tuning and regularization concepts, such as Bias-Variance Tradeoff, Randomized Hyperparameter Search, $\ell_1$ and $\ell_2$ regularization, Dropout, and Batch Normalization.
		\item Proficient in fundamental machine learning algorithms, such as Linear 
		Regression and its variants (Ridge, Lasso, and Elastic Net Regression), Logistic and Softmax Regression, Support Vector Machine (SVM), Decision Tree, Random Forest, Naive Bayes Classifier, Bayesian Network, Ensemble Learning, XGBoost, Principal Component Analysis (PCA), and $k$-means clustering.
	\end{itemize}

	% & English (Fluent. IELTS overall band score: 8.0 out of 9.0, taken in Oct 2019)\\
	% & Chinese (Native)
	% \\\\
	
	% \pbox{2.1cm}{Programming}
	% & Python (Proficient)\\
	% & Data science packages, such as NumPy, Pandas, Matplotlib, Scikit-learn, Keras, and TensorFlow (Proficient)
	
	% \pbox{2.1cm}{Machine learning}
	% & Proficient in deep learning models, such as Multilayer Perceptron (MLP), Convolutional Neural Network (CNN), and Recurrent Neural Network (RNN).\\
	% % \vspace{10cm}
	% & Proficient in hyperparameter tuning and regularization concepts, such as Randomized Hyperparameter Search, $\ell_1$ and $\ell_2$ regularization, Dropout, Batch Normalization, \\
	% & Proficient in fundamental machine learning algorithms, such as Linear Regression and its variants (Ridge, Lasso, and Elastic Net Regression), Logistic and Softmax Regression, Support Vector Machine (SVM), Decision Tree, Random Forest, Naive Bayes Classifier, Principal Component Analysis (PCA), and $k$-means clustering.\\
	% \\\\
\end{tabularx}
%\end{tabular}
%\end{table}


% \section*{Language and Technical Skills}
% % Languages: English (Fluent; IELTS overall band score: 8.0 out of 9.0), Cantonese (Native), Mandarin (Intermediate)
% Languages:
% \begin{itemize}
% 	\item English (Fluent. IELTS overall band score: 8.0 out of 9.0, taken in Oct 2019)
% 	\item Chinese (Native)
% 	% \item Mandarin (Intermediate)
% \end{itemize}
% Programming languages:
% \begin{itemize}
% 	\item Python (Proficient)
% 	\item Data science packages, such as NumPy, Pandas, Scikit-learn, Keras, and TensorFlow (Proficient)
% 	% \item C++ (Intermediate)
% \end{itemize}
% % Technical knowledge:
% % deep learning: 






%\section*{Work Experience}
%\begin{itemize}
%%	\begin{comment}
%		\item \textbf{Assistant Lecturer}, Hong Kong Community College (HKCC), Hong Kong Polytechnic University (Aug 2017--Now):
%	\begin{itemize}
%		\item SEHH1048: Introduction to Linear Algebra (Fall 17, Spring 18, Spring 19)
%		\item SEHH1050: Introduction to Probability and Statistics (Fall 17)		
%		\item SEHH1068: Foundation Mathematics (Fall 19)		
%		\item SEHH1069: Calculus and Linear Algebra (Spring 19)                              
%		\item SEHH2042: Computer Programming (C++) (Fall 18)
%		\item SEHH2241: Discrete Structures (Fall 18, Fall 19)
%		\item Programming Workshop (Winter 17, Winter 18, Winter 19)
%		\item Calculus Workshop (Fall 17, Spring 18)
%		\item Tutor at Math Learning Center (Fall 17, Spring 18)
%	\end{itemize}
%%	\end{comment}
%	\item \textbf{Teaching Assistant}, HKUST (Feb 2016--Jun 2017):
%	\begin{itemize}
%		\item COMP1942: Exploring and Visualizing Data (Spring 17)
%		\item COMP2711: Discrete Mathematical Tools for Computer Science (Fall 16) %\vspace{0.1cm}\\\footnotesize{Teaching tutorials, grading quizzes \& exams, and holding consultation sessions. }
%		\item COMP3511: Database Management Systems (Spring 16)
%		%\vspace{0.1cm}\\\footnotesize{Teaching labs, setting homework \& exam questions, grading homework \& exams, and holding consultation sessions. }
%	\end{itemize}
%\end{itemize}





%	\item Eliminated assumptions on hidden bipartite graphs, whereby proposed a new research problem entitled \textit{hidden frequent item finding}, which bears a strong relationship with the frequent item mining problem in data mining.
%	\item Designed fast sampling algorithms to address the newly proposed problem.
%	\item Research findings submitted to a competition organized by the IEEE Computational Intelligence Chapter and received the Champion Award.
%
%\end{itemize}
%\item \textbf{Topic modeling on Amazon Discussions Forum} (Sep 2014--May 2015; Final Year Project):
%\begin{itemize}
%	%automatically organize the posts on
%	\item Applied latent Dirichlet allocation (LDA) to the Amazon Discussions Forum so that users can browse this large online forum much more efficiently. %\href{https://www.amazon.com/forum/amazon\%20discussions%20feedback}{Amazon Discussions Forum}.
%	\item Automatically classified the unstructured forum posts to different topics by using LDA and subsequently displayed the results on a website, through which users can swiftly identify the posts of their interest.
%\end{itemize}
%\end{itemize}



% \section*{Proposed Research Title}
% Finding the most connected vertices in a hidden graph via random sampling.
% \section*{Proposed Research Project Summary}
% An undirected graph is said to be hidden if the edges of the graph are not explicitly given. The problem of detecting the edges of a hidden bipartite graph has recently generated considerable interest in the database research community because of its many applications. All previous studies on this problem assume that to detect the presence of an edge between two given vertices, one must perform an \new{edge-probing query}, which returns a Boolean answer as to whether the two vertices are connected. In many real-world problems, however, such a query may not exist. Instead, we may at best receive only a \new{score} after performing the query. 

% In this project, we study the hidden bipartite graph problem, but with the additional constraint that to discover the existence of an edge, merely performing one edge-probing query is insufficient. Instead, we assume at our disposal a \new{score-probing query}, which returns the score of a vertex with respect to another vertex. Specifically, given a hidden bipartite graph $G$ between two sets of vertices $\B$ (called black vertex set) and $\W$ (called white vertex set), we aim to identify the $k$ vertices with the highest degrees from the white vertex set $\W$. An edge between two vertices, say $b\in\B$ and $w\in\W$, is established if and only if vertex $w$ receives the highest score from the score function of vertex $b$, relative to other vertices in $\W$. The challenge lies in solving this problem while keeping the number of score-probing queries required to a minimum. 
% \begin{comment}
% An edge is established only if one vertex has the highest score with respect to all vertices in another independent set of the given bipartite graph. Our aim is to identify the $k$ vertices with the highest degrees, while keeping the number of score-probing queries required to a minimum. 

% Therefore, rather than assume the availability of an edge-probing query, we instead assume at our disposal a \new{score-probing query}, which returns the score of a vertex with respect to another vertex. Specifically, given a hidden bipartite graph $G$ between two sets of vertices $\B$ (called black vertex set) and $\W$ (called white vertex set), we aim to identify the $k$ vertices with the highest degrees from the white vertex set $\W$. An edge between two vertices, say $b\in\B$ and $w\in\W$, is established if and only if vertex $w$ receives the highest score from the score function of vertex $b$, relative to other vertices in $\W$. The challenge lies in solving this problem while keeping the number of score-probing queries required to a minimum. 



% \section*{Work Experience}
% \vspace{-0.5cm}
% \begin{table}[!h]
% 	\begin{tabular}{l|p{13cm}}	
% 		\toprule 
% 		\textsc{Jul} 2007 |& Store Assistant\\
% 		\textsc{Sep} 2009& Taste, Olympian City 2, 18 Hoi Ting Road, West Kowloon, Hong Kong
% 	\end{tabular}
% \end{table}




\section*{Relevant Coursework}
%\vspace{ -.4 cm}
%\begin{table}
%\begin{tabular}{p{2.1cm}|p{14cm}}
%\begin{longtable}{l|p{16cm}}
\begin{tabularx}{\linewidth}{p{2.8cm}|p{7cm} p{9cm}}
	\toprule 
	Data Science
	% \small{Data Science}
	% \pbox{2.1cm}{Languages} 
	&
	Undergraduate Machine learning (A+) & Postgraduate Machine learning (A)\\
	&
	Introduction to Bayesian Networks (A) & Exploring and Visualizing Data (A+)\\
	% \\\\
	% Statistics
	% \pbox{2.1cm}{Programming languages}
	&
	Regression Analysis (A+) & Time Series Forecasting (A+)\\
	&
	Statistical Data Analysis (A+) & Survey Design and Analysis (A+)\\	
	&
	Statistics I (A+) & Statistics II (A+) \\
	&
	Economic and Social Statistics (A+) 
	\\\\
	Mathematics &
	% \small{Mathematics} &
	Linear Algebra (A+) & Introduction to Multivariable Calculus (A+)\\
	& Financial Mathematics (A+) & Discrete Mathematics for Computer Science (A+)\\
	& Logic (A+)
	\\\\
	Computer Science
	&
	% \small{Computer Science}&
	Principles of Programming (A+) & Object-Oriented Programming and Data Structures (A+)\\
	& Design and Analysis of Algorithms (A) & Introduction to Advanced Algorithmic Techniques (A-)\\
	& Fundamentals of Database Systems (A) & Cryptography and Security (A+)\\
	& Operating Systems (A) & Computer Organization (A-)\\
\end{tabularx}
%\end{tabular}
%\end{table}

% \section*{References}
% \begin{tabularx}{\linewidth}{p{9cm} p{9cm}}
% 	\toprule 
% \textbf{Prof. \href{https://www.cse.ust.hk/~raywong/}{Raymond Chi-Wing Wong}, Professor}& 	\b{Dr. \href{https://www.hkcc-polyu.edu.hk/en/about-hkcc/staff-directory/division-of-science-engineering-and-health-studies/index.php?sid=184}{Anthony Wai-Keung Loh}, Division Head}\\
% Department of Computer Science and Engineering&	Division of Science, Engineering and Health Studies\\
% Hong Kong University of Science and Technology&	College of Professional and Continuing Education\\
% Tel: (852) 2358-6982& Hong Kong Polytechnic University\\
% Email: \href{raywong@cse.ust.hk}{raywong@cse.ust.hk}&	Tel: (852) 3746-0238\\
% &	Email: \href{anthony.wk.loh@cpce-polyu.edu.hk}{anthony.wk.loh@cpce-polyu.edu.hk}\\
% \\
% \b{Dr. \href{https://www.hkcc-polyu.edu.hk/en/about-hkcc/staff-directory/division-of-science-engineering-and-health-studies/index.php?sid=213}{Ching-On Lo}, Senior Lecturer} & \b{Dr. \href{https://www.hkcc-polyu.edu.hk/en/about-hkcc/staff-directory/division-of-science-engineering-and-health-studies/index.php?sid=213}{Wilson Chun-Kit Kwan}, Senior Lecturer}\\
% Division of Science, Engineering and Health Studies & Division of Science, Engineering and Health Studies\\
% College of Professional and Continuing Education & College of Professional and Continuing Education\\
% Hong Kong Polytechnic University & Hong Kong Polytechnic University\\
% Tel: (852) 3746-0626 & Tel: (852) 3746-0258\\
% Email: \href{mailto:co.lo@cpce-polyu.edu.hk}{co.lo@cpce-polyu.edu.hk} & Email: \href{wilson.kwan@cpce-polyu.edu.hk}{wilson.kwan@cpce-polyu.edu.hk}\\
% \end{tabularx}

\section*{Publication}
\begin{itemize}
	\item Joe Wing-Ho Lin and Raymond Chi-Wing Wong. \href{https://link.springer.com/chapter/10.1007/978-3-030-27520-4_4}{Frequent Item Mining when Obtaining Support is Costly}. \textit{In 21st International Conference on Big Data Analytics and Knowledge Discovery} (DaWaK), pages 37--56. Springer, 2019.	
	% \vspace*{-0.5cm}
	% \paragraph{Abstract:} suppose there are $n$ users and $m$ items, and the preference of each user for the items is revealed only upon probing, which takes time and is therefore costly. How can we quickly discover all the frequent items that are favored individually by at least a given number of users? This new problem not only has strong connections with several well-known problems, such as the frequent item mining problem, it also finds applications in fields such as sponsored search and marketing surveys. Unlike traditional frequent item mining, however, our problem assumes no prior knowledge of users' preferences, and thus obtaining the support of an item becomes costly. Although our problem can be settled naively by probing the preferences of all $n$ users, the number of users is typically enormous, and each probing itself can also incur a prohibitive cost. 
	
	% We present a sampling algorithm that drastically reduces the number of users needed to probe to $O(\log m)$---regardless of the number of users---as long as slight inaccuracy in the output is permitted. For reasonably sized input, our algorithm needs to probe only $0.5\%$ of the users, whereas the naive approach needs to probe all of them. 
\end{itemize}

% \section*{Professional Experience}
% \begin{itemize}
% 	\item \textbf{Paper Reviewer} of International Conferences:
% 	\begin{itemize}
% 		\item International Conference on Data Mining (ICDM) (2016 \& 2017)
% 		\item Conference on Information and Knowledge Management (CIKM) (2016 \& 2017)
% 		\item SIAM International Conference on Data Mining (SDM) (2017)
% 		\item International Conference on Data Science and Advanced Analytics (DSAA) (2016)	
% 		\item Asia Pacific Web Conference (APWeb) (2016)
% 	\end{itemize}
% \end{itemize}


% \\ (Among 6 out of 61 papers invited to appear in the special issue of Journal of Data Intelligence)

\end{document}

